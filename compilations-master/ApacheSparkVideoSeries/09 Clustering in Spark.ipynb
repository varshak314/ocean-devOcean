{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#create session\n",
    "appName = \"Clustering in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file into dataFrame\n",
    "Description for each column data:\n",
    "- CustomerName: name of customer\n",
    "- Age: age of customer (in year)\n",
    "- MaritalStatus: (1=married, 0=not married)\n",
    "- IncomeRange: income per year (in USD)\n",
    "- Gender: (1=female, 2=male)\n",
    "- TotalChildren: number of children customer has\n",
    "- ChildrenAtHome: number of children living with customer (in the same home)\n",
    "- Education: (1=high school, 2=bachelor, 3=master, 4=PhD, 5=Post-doc)\n",
    "- Occupation: (0=unskilled manual work until 5=professional)\n",
    "- HomeOwner: (1=owning a home, 0=not owning a home)\n",
    "- Cars: number of car customer has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "|   CustomerName|Age|MaritalStatus|IncomeRange|Gender|TotalChildren|ChildrenAtHome|Education|Occupation|HomeOwner|Cars|\n",
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "|    Aaron Adams| 42|            0|      50000|     0|            0|             0|        3|         2|        1|   1|\n",
      "|Aaron Alexander| 40|            1|      50000|     0|            0|             0|        2|         2|        1|   2|\n",
      "|    Aaron Allen| 63|            0|      25000|     0|            2|             1|        2|         1|        1|   2|\n",
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file using automatically inferred schema\n",
    "customers = spark.read.csv(\n",
    "    'dataset/customers.csv', inferSchema=True, header=True)\n",
    "customers.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------------------------------+\n",
      "|CustomerName   |features                                      |\n",
      "+---------------+----------------------------------------------+\n",
      "|Aaron Adams    |[42.0,0.0,50000.0,0.0,0.0,0.0,3.0,2.0,1.0,1.0]|\n",
      "|Aaron Alexander|[40.0,1.0,50000.0,0.0,0.0,0.0,2.0,2.0,1.0,2.0]|\n",
      "|Aaron Allen    |[63.0,0.0,25000.0,0.0,2.0,1.0,2.0,1.0,1.0,2.0]|\n",
      "+---------------+----------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define assembler\n",
    "assembler = VectorAssembler(inputCols = [\n",
    "    \"Age\", \"MaritalStatus\", \"IncomeRange\", \"Gender\", \"TotalChildren\", \n",
    "    \"ChildrenAtHome\", \"Education\", \"Occupation\", \"HomeOwner\", \"Cars\"], \n",
    "                            outputCol=\"features\")\n",
    "data = assembler.transform(customers).select('CustomerName', 'features')\n",
    "data.show(truncate = False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create k-Means clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is successfully trained!\n"
     ]
    }
   ],
   "source": [
    "#define kMeans clustering algorithm\n",
    "kmeans = KMeans(\n",
    "    featuresCol=assembler.getOutputCol(), \n",
    "    predictionCol=\"cluster\", k=5)\n",
    "model = kmeans.fit(data)\n",
    "print (\"Model is successfully trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print centroid for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[5.60711289e+01 5.83804487e-01 7.50000000e+04 5.03921211e-01\n",
      " 2.17308043e+00 8.16706183e-01 3.73244574e+00 3.92759438e+00\n",
      " 7.23326646e-01 1.38063104e+00]\n",
      "[5.31013005e+01 4.17180014e-01 2.50000000e+04 4.80492813e-01\n",
      " 1.41512663e+00 6.08487337e-01 2.31622177e+00 1.45448323e+00\n",
      " 5.93086927e-01 1.11464750e+00]\n",
      "[5.53417813e+01 5.72411296e-01 1.00000000e+05 4.97103548e-01\n",
      " 2.54380883e+00 1.54272266e+00 3.46198407e+00 4.19116582e+00\n",
      " 7.16509776e-01 1.94532947e+00]\n",
      "[5.82794840e+01 6.22850123e-01 1.50000000e+05 4.79729730e-01\n",
      " 2.07248157e+00 3.20638821e+00 3.41461916e+00 4.34705160e+00\n",
      " 6.48648649e-01 3.10995086e+00]\n",
      "[5.19737441e+01 5.26868545e-01 5.00000000e+04 4.93961141e-01\n",
      " 1.34552774e+00 4.98337126e-01 3.23035183e+00 2.77927534e+00\n",
      " 6.62699107e-01 1.14615789e+00]\n"
     ]
    }
   ],
   "source": [
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|cluster|count|\n",
      "+-------+-----+\n",
      "|      0| 5483|\n",
      "|      1| 2922|\n",
      "|      2| 2762|\n",
      "|      3| 1628|\n",
      "|      4| 5713|\n",
      "+-------+-----+\n",
      "\n",
      "+---------------+-------+\n",
      "|   CustomerName|cluster|\n",
      "+---------------+-------+\n",
      "|    Aaron Adams|      4|\n",
      "|Aaron Alexander|      4|\n",
      "|    Aaron Allen|      1|\n",
      "|    Aaron Baker|      4|\n",
      "|   Aaron Bryant|      0|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(data)#cluster given data\n",
    "prediction.groupBy(\"cluster\").count().orderBy(\"cluster\").show()#count members in each cluster\n",
    "prediction.select('CustomerName', 'cluster').show(5)#show several clustered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
